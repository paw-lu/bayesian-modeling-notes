# Markov Chain Monte Carlo

## MCMC

_Markov Chain Monte Carlo_ (_MCMC_).
Knowing the process helps to figure out if algorithm has converged
and understand why we get thousand of samples as a solution.

### Bayesian landscape

When a Bayesian problem with $N$ unknowns
we create an $N$ dimensional space for the prior distribution to exist in.
On top of that space is an additional dimension—
the _surface_ or _curve_ that reflects the _prior probability_ of a particular point.
This surface is defined by prior distributions
If we have two unknowns—
$p_1$ and $p_2$—
and priors for both are $\text{Uniform}(0, 5)$,
the space created is a square of length 5
and the surface is a flat plane
that sits on top of the square.
Every point is equally likely.

![Prior surface](images/3/prior_surface.png)

If the two priors are $\text{Exp}(3)$ and $\text{Exp}(10)$,
then the space is all positive numbers on the 2D plane
and the surface slopes down from $(0, 0)$.

![Exponential prior surface](images/3/exponential_prior_surface.png)

In practice,
these spaces can be of a much higher dimension.

What happens to this space after the data $X$ is incorporated?
The data $X$ does not change the space,
but it does change the surface by pulling and stretching the fabric of the prior surface
to reflect where the true parameters live.
More data means more distortion,
and original shape becomes insignificant.
The resulting surface describes the _posterior distribution_.

For the simple 2D case the data pushes up the surface to make peaks.
If there was already a peak there due to the prior,
it gets pushed higher.
If the prior has an assigned probability of 0
then no posterior probability will be assigned there.

![Distribution warping](images/3/distribution_warping.png)

Even though same data was used to warp,
the result looks different on the two columns.
This influences the warping.

#### MCMC landscape

We should explored the posterior space
generated by our prior surface and observed data
to find the posterior mountain.
However searching a $N$ dimensional space is exponentially difficult in $N$.
MCMC performs an intelligent search of the space.

MCMC returns _samples_ from the posterior distribution,
not the distribution itself.
MCMC asks
_how likely is it for a point to be from the mountain we are searching for?_.
These points are samples,
or _traces_.

MCMC hopefully converges towards an area of high posterior probability.
MCMC accomplishes this by exploring nearly positions
and moving into areas with a higher probability.

#### Why thousand of samples?

Returning a mathematical formula for the mountain peaks
would involve describing an $N$ dimensional surface with arbitrary peaks and valleys.

Returning the peak of a landscape is possible—
as the highest point corresponds to the most probable estimate of unknowns.

But this ignores the shape of the landscape,
which is very important in determining confidence intervals.

Returning samples allows for the use of _the law of large numbers_ to solve problems.
With thousand of samples we can reconstruct the posterior surface
by organizing them in a histogram.
